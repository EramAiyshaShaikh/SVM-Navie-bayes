{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#ASSIGNMENT QUESTIONS"
      ],
      "metadata": {
        "id": "aqKj2lNy6oxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 : What is Information Gain, and how is it used in Decision Trees?**\n",
        "\n",
        "Answer:\n",
        "- Information Gain measures how much a feature reduces the uncertainty (entropy) in a dataset, and in decision trees, it is used to select the best feature to split the data at each node. The feature with the highest Information Gain is chosen because it creates the most homogeneous subsets, leading to a more accurate tree.\n",
        "- How Information Gain is used in Decision Trees:\n",
        "  - Measuring uncertainty: At each node of the tree, the algorithm calculates the entropy of the dataset. Entropy is a measure of the impurity or randomness in the data, with higher entropy indicating more mixed-up classes.\n",
        "  - Calculating the gain: For each potential feature, the algorithm calculates the Information Gain that would result from splitting the data based on that feature.\n",
        "  - Selecting the best split: The Information Gain for a feature is calculated as the initial entropy of the dataset minus the weighted average entropy of the subsets created after the split.\n",
        "  - Gain(S,A)=Entropy(S)−∑\n",
        "v\n",
        "A\n",
        "​\n",
        "  ∣Sv∣/\n",
        "∣S\n",
        "​\n",
        " ∣\n",
        "​\n",
        " .Entropy(S\n",
        "v\n",
        "​\n",
        " )\n",
        "  - Choosing the root node and subsequent nodes: The feature with the highest Information Gain is chosen as the splitting attribute for the current node. This greedy approach is repeated at each new node until the tree is complete.\n",
        "\n",
        "**Question 2: What is the difference between Gini Impurity and Entropy?**\n",
        "\n",
        "Answer:\n",
        "- Gini Impurity:\n",
        "  - It is the probability of misclassifying a randomly chosen element in a set.\n",
        "  - The range of the Gini index is [0, 0.5], where 0 indicates perfect purity and 0.5 indicates maximum impurity.\n",
        "  - Gini index is a linear measure.\n",
        "  - It can be interpreted as the expected error rate in a classifier.\n",
        "  - It is sensitive to the distribution of classes in a set.\n",
        "  - The computational complexity of the Gini index is O(c).\n",
        "  - It is less robust than entropy.\n",
        "  - It is sensitive.\n",
        "  - Formula for the Gini index is\n",
        "    Gini = 1−∑C x=1 px 2\n",
        "    ​, where\n",
        "    p\n",
        "    x\n",
        "      is\n",
        "\n",
        "    the proportion of the instances of class x in a set.\n",
        "  - It has a bias toward selecting splits that result in a more balanced distribution of classes.\n",
        "  - Gini index is typically used in CART (Classification and Regression Trees) algorithms\n",
        "\n",
        "- Entropy:\n",
        "  - Entropy measures the amount of uncertainty or randomness in a set.  \n",
        "  - The range of entropy is [0, log2(C)], where c is the number of classes. The range becomes [0, 1] for binary classification.\n",
        "  - Entropy is a logarithmic measure.\n",
        "  - It can be interpreted as the average amount of information needed to specify the class of an instance.\n",
        "  - It is sensitive to the number of classes.\n",
        "  - Computational complexity of entropy is O(c * log(c)).\n",
        "  - It is more robust than Gini index.\n",
        "  - It is comparatively less sensitive.\n",
        "  - Formula for entropy is\n",
        "    Entropy =\n",
        "    −∑x = 1Cpxlog⁡2(px)\n",
        "    ,\n",
        "    where px is the proportion of the instances of class x in a set.  \n",
        "  - It has a bias toward selecting splits that result in a higher reduction of uncertainty.\n",
        "  - Entropy is typically used in ID3 and C4.5 algorithms\n",
        "\n",
        "**Question 3:What is Pre-Pruning in Decision Trees?**\n",
        "\n",
        "Answer:\n",
        "- Pre-pruning, or early stopping, is a technique in decision trees that halts the growth of a tree before it is fully developed to prevent overfitting. This is done by setting criteria that stop the splitting process at a node, such as setting a maximum tree depth, minimum number of samples per split or leaf, or a minimum information gain threshold.\n",
        "- How it works:\n",
        "  - Early stopping: Instead of building a complete, potentially overfit tree and then simplifying it, pre-pruning stops the growth during the tree-building process.\n",
        "  - Criteria for stopping: The growth is stopped when certain conditions are met, which are set as hyperparameters to control the model's complexity.\n",
        "  - Examples of criteria:\n",
        "    - Maximum depth: Restricting the maximum number of levels in the tree.\n",
        "    - Minimum samples per split: Requiring a minimum number of samples to split a node.\n",
        "    - Minimum samples per leaf: Specifying a minimum threshold for the number of samples in each final leaf node.\n",
        "    - Minimum information gain: Stopping the split if no attribute satisfies a minimum information gain or impurity decrease.   \n",
        "    \n",
        "      "
      ],
      "metadata": {
        "id": "rQbdBKof6xi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "Answer:'''\n",
        "\n",
        "#import all the libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#take out independent and dependent variables\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "#split variavles for training\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "#create the object of decision tree classifier\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "\n",
        "#train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#get feature importance\n",
        "FI = clf.feature_importances_\n",
        "\n",
        "#create a dataframe to display the features\n",
        "df = pd.DataFrame({'Feature':wine.feature_names, 'Importance':FI})\n",
        "\n",
        "#sort the values to get the features according to priority\n",
        "df = df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "#print the dataframe\n",
        "print(df)\n",
        "print(f\"So the most import feature is: {df['Feature'].iloc[0]}\")\n",
        "\n",
        "#printing the accuracy score\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f\"The accurcy is:{accuracy_score(y_test, y_pred)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkTZ3PDIet0z",
        "outputId": "21ba3f9d-2601-4e49-c928-eb917b6915c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         Feature  Importance\n",
            "12                       proline    0.406853\n",
            "6                     flavanoids    0.371834\n",
            "11  od280/od315_of_diluted_wines    0.105571\n",
            "9                color_intensity    0.067982\n",
            "1                     malic_acid    0.024111\n",
            "10                           hue    0.023648\n",
            "3              alcalinity_of_ash    0.000000\n",
            "2                            ash    0.000000\n",
            "0                        alcohol    0.000000\n",
            "4                      magnesium    0.000000\n",
            "8                proanthocyanins    0.000000\n",
            "5                  total_phenols    0.000000\n",
            "7           nonflavanoid_phenols    0.000000\n",
            "So the most import feature is: proline\n",
            "The accurcy is:0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What is a Support Vector Machine (SVM)?**\n",
        "\n",
        "Answer:\n",
        "- A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression that works by finding the optimal \"hyperplane\" or decision boundary that best separates data points into different classes. The goal is to maximize the margin, which is the distance between the hyperplane and the closest data points of each class. These closest points are called support vectors because they define the margin.\n",
        "- Key characteristics:\n",
        "  - Supervised learning: It requires labeled training data to learn how to classify or predict.\n",
        "  - Classification: It is most commonly used for classification, especially binary classification (e.g., spam vs. not spam).\n",
        "  - Regression: It can also be used for regression tasks.\n",
        "  - Hyperplane: The \"decision boundary\" is a hyperplane in a high-dimensional space. In a 2D space, it's a line; in a 3D space, it's a plane.\n",
        "  - Maximizing the margin: The algorithm finds the hyperplane that has the largest possible distance to the nearest data points of any class, making it robust to new data.\n",
        "  - Support vectors: The data points on the edge of the margin that \"support\" the hyperplane are called support vectors.\n",
        "  - Kernel trick: For data that is not linearly separable, SVMs can use a kernel function to transform the data into a higher-dimensional space where a linear separation is possible, without explicitly computing the transformation.\n",
        "  - Versatility: SVMs are versatile and can handle high-dimensional data and are effective even when the number of dimensions is greater than the number of samples.\n",
        "\n",
        "\n",
        "**Question 6: What is the Kernel Trick in SVM?**\n",
        "\n",
        "Answer:\n",
        "- The Kernel Trick is a technique used in Support Vector Machines (SVMs) to handle non-linear data by implicitly mapping it into a higher-dimensional space where it becomes linearly separable. Instead of explicitly calculating the data's coordinates in this new, higher-dimensional space, a kernel function computes the dot product directly, which is computationally efficient and avoids the cost of transforming the data. This allows SVMs to find a linear separation for data that is not linearly separable in its original low-dimensional space.\n",
        "- How it works:\n",
        "  - The Problem: Non-linear data that cannot be separated by a straight line (a linear classifier) in its original space.\n",
        "  - The Solution: Use a kernel function to transform the data into a higher-dimensional space where it is possible to find a linear separating plane.\n",
        "  - The \"Trick\": Avoid the explicit and computationally expensive step of mapping the data to the higher-dimensional space. Instead, the kernel function, which takes two input vectors and returns their dot product in the higher-dimensional space, is applied directly to the original data.\n",
        "  - The Result: A linear classifier can be used to find a separation in the new, higher-dimensional feature space, effectively solving the non-linear problem in the original space.\n",
        "- Example kernel functions:\n",
        "  - Polynomial kernel: Maps data to a higher-dimensional space using polynomial functions.\n",
        "  - Radial Basis Function (RBF) kernel: Another common kernel that can map data to an infinite-dimensional space.\n",
        "  - Linear kernel: The most basic kernel, used for linearly separable data.\n",
        "  "
      ],
      "metadata": {
        "id": "yNxoCRf2FeWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "Answer:'''\n",
        "\n",
        "#import all the libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#take out independent and dependent variables\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "#split the variables for training\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "#-------------------FOR LINEAR SVC----------------------------------------------\n",
        "#make the object of support vector classifier\n",
        "clf = SVC(kernel='linear')\n",
        "\n",
        "#built the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#predict the model\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "#evaluate the model\n",
        "accuracy_linear = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of SVM with Linear Kernel: {accuracy_linear:.2f}\")\n",
        "\n",
        "#---------------------FOR RBF SVC-----------------------------------------------\n",
        "#make the object of support vector classifier\n",
        "clf = SVC(kernel='rbf')\n",
        "\n",
        "#built the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#predict the model\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "#evaluate the model\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of SVM with RBF Kernel: {accuracy_rbf:.2f}\")\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#comparing the accuraccies\n",
        "if accuracy_linear > accuracy_rbf:\n",
        "    print(\"Linear kernel performs better.\")\n",
        "elif accuracy_rbf > accuracy_linear:\n",
        "    print(\"rbf kernel performs better.\")\n",
        "else:\n",
        "    print(\"Both SVMs performed equally well.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V81-U6N7R6ng",
        "outputId": "0a218dc3-91f8-405c-a086-79d0e9ec63de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear Kernel: 0.96\n",
            "Accuracy of SVM with RBF Kernel: 0.69\n",
            "Linear kernel performs better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**\n",
        "\n",
        "Answer:\n",
        "- Naïve Bayes algorithm is a supervised learning algorithm, which is based on Bayes theorem and used for solving classification problems.\n",
        "- It is mainly used in text classification that includes a high-dimensional training dataset.\n",
        "- Naïve Bayes Classifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make quick predictions.\n",
        "- It is a probabilistic classifier, which means it predicts on the basis of the probability of an object.\n",
        "- Some popular examples of Naïve Bayes Algorithm are spam filtration, Sentimental analysis, and classifying articles.\n",
        "- Why is it called Naïve Bayes?\n",
        "  - The Naïve Bayes algorithm is comprised of two words Naïve and Bayes, Which can be described as:\n",
        "  - Naïve: It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identify that it is an apple without depending on each other.\n",
        "  - Bayes:It is called Bayes because it depends on the principle of Bayes' Theorem\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "**Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes**\n",
        "\n",
        "Answer:\n",
        "- The key difference between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes lies in the type of data they handle: Gaussian Naïve Bayes assumes continuous features follow a normal distribution, Multinomial Naïve Bayes works with discrete counts, and Bernoulli Naïve Bayes deals with binary features. Here's a breakdown:\n",
        "  - Gaussian Naïve Bayes:\n",
        "    - Data Type: Assumes continuous features\n",
        "    - Distribution: Uses a Gaussian (normal) distribution for each feature\n",
        "    - Use Case: Suitable for numerical data like age or temperature\n",
        "  - Multinomial Naïve Bayes:\n",
        "    - Data Type: Assumes features are discrete counts\n",
        "    - Distribution: Uses a multinomial distribution\n",
        "    - Use Case: Commonly used in text classification where features represent word counts\n",
        "  - Bernoulli Naïve Bayes:\n",
        "    - Data Type: Assumes features are binary (either 0 or 1)\n",
        "    - Distribution: Uses a Bernoulli distribution\n",
        "    - Use Case: Often used for tasks like spam detection where features represent the presence or absence of words\n"
      ],
      "metadata": {
        "id": "Op5wfckkGwC3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXBDTB_zQx5C",
        "outputId": "4bb9aadd-a43b-4b2e-8ab2-ec5e6629bfec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9473684210526315"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "\n",
        "Answer:'''\n",
        "#import the breast cancer dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "#import train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "#import Gaussian and make object\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf = GaussianNB()\n",
        "\n",
        "#fit the model using X_train and y_train from train test split\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "#predict the test data from the trained model above\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "#import accuracy score to evaluate the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred)"
      ]
    }
  ]
}